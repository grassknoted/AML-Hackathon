{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os.path import join\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../../Hackathon/Selfie-dataset/selfie_dataset.txt\"#join(\"..\", \"..\", \"Dataset-1\", \"selfie_dataset.txt\")\n",
    "image_path = \"../../Hackathon/Selfie-dataset/images/\"#join(\"..\", \"..\", \"Dataset-1\", \"selfie_dataset.txt\")#join(\"..\", \"..\", \"Dataset-1\", \"images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>score</th>\n",
       "      <th>partial_faces</th>\n",
       "      <th>is_female</th>\n",
       "      <th>baby</th>\n",
       "      <th>child</th>\n",
       "      <th>teenager</th>\n",
       "      <th>youth</th>\n",
       "      <th>middle_age</th>\n",
       "      <th>senior</th>\n",
       "      <th>...</th>\n",
       "      <th>curly_hair</th>\n",
       "      <th>straight_hair</th>\n",
       "      <th>braid_hair</th>\n",
       "      <th>showing_cellphone</th>\n",
       "      <th>using_earphone</th>\n",
       "      <th>using_mirror</th>\n",
       "      <th>braces</th>\n",
       "      <th>wearing_hat</th>\n",
       "      <th>harsh_lighting</th>\n",
       "      <th>dim_lighting</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00a454da495e11e28a7322000a1fa414_6</td>\n",
       "      <td>3.901</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00cddb96ac4c11e3a30212279ba1b65f_6</td>\n",
       "      <td>4.385</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01cdd7aa1a1a11e2aaa822000a1fb0dd_6</td>\n",
       "      <td>4.243</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           image_name  score  partial_faces  is_female  baby  \\\n",
       "0  00a454da495e11e28a7322000a1fa414_6  3.901              1          1    -1   \n",
       "1  00cddb96ac4c11e3a30212279ba1b65f_6  4.385              1          1    -1   \n",
       "2  01cdd7aa1a1a11e2aaa822000a1fb0dd_6  4.243             -1          1    -1   \n",
       "\n",
       "   child  teenager  youth  middle_age  senior      ...       curly_hair  \\\n",
       "0     -1        -1      1          -1      -1      ...               -1   \n",
       "1     -1        -1     -1          -1      -1      ...               -1   \n",
       "2     -1         1     -1          -1      -1      ...               -1   \n",
       "\n",
       "   straight_hair  braid_hair  showing_cellphone  using_earphone  using_mirror  \\\n",
       "0             -1          -1                 -1              -1            -1   \n",
       "1             -1          -1                 -1              -1            -1   \n",
       "2             -1          -1                 -1              -1            -1   \n",
       "\n",
       "   braces  wearing_hat  harsh_lighting  dim_lighting  \n",
       "0      -1           -1              -1            -1  \n",
       "1      -1           -1              -1            -1  \n",
       "2      -1           -1              -1            -1  \n",
       "\n",
       "[3 rows x 38 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers = [\n",
    "    \"image_name\", \"score\", \"partial_faces\" ,\"is_female\" ,\"baby\" ,\"child\" ,\"teenager\" ,\"youth\" ,\"middle_age\" ,\"senior\" ,\"white\" ,\"black\" ,\"asian\" ,\"oval_face\" ,\"round_face\" ,\"heart_face\" ,\"smiling\" ,\"mouth_open\" ,\"frowning\" ,\"wearing_glasses\" ,\"wearing_sunglasses\" ,\"wearing_lipstick\" ,\"tongue_out\" ,\"duck_face\" ,\"black_hair\" ,\"blond_hair\" ,\"brown_hair\" ,\"red_hair\" ,\"curly_hair\" ,\"straight_hair\" ,\"braid_hair\" ,\"showing_cellphone\" ,\"using_earphone\" ,\"using_mirror\", \"braces\" ,\"wearing_hat\" ,\"harsh_lighting\", \"dim_lighting\"\n",
    "]\n",
    "df_image_details = pd.read_csv(data_path, names=headers, delimiter=\" \")\n",
    "df_image_details.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_image_details = df_image_details[df_image_details.is_female != 0]\n",
    "df_image_details.replace(to_replace=-1, value=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names = df_image_details.image_name.values.copy()\n",
    "image_scores = df_image_details[headers[1]].values.copy()\n",
    "image_attrs = df_image_details[headers[2:]].values.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = [join(image_path, iname) + '.jpg' for iname in image_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths_train, image_paths_test = image_paths[:-1000], image_paths[-1000:]\n",
    "image_attrs_train, image_attrs_test = image_attrs[:-1000], image_attrs[-1000:]\n",
    "image_scores_train, image_scores_test = image_scores[:-1000], image_scores[-1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import Sequence\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageGenerator(Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y1 = self.y[0][idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y2 = self.y[1][idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "\n",
    "        # read your data here using the batch lists, batch_x and batch_y\n",
    "        x = [self.read_image(filename) for filename in batch_x] \n",
    "        y1 = [atrributes for atrributes in batch_y1]\n",
    "        y2 = [atrributes for atrributes in batch_y2]\n",
    "        return [np.array(x), np.array(x)], [np.array(y1), np.array(y2)]\n",
    "    \n",
    "    def read_image(self, fname):\n",
    "        im = cv2.imread(fname)\n",
    "        im = cv2.resize(im, (224, 224), interpolation=cv2.INTER_CUBIC)\n",
    "        return im / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import resnet50\n",
    "from keras.layers import Dense, Conv2D, MaxPool2D, Input, Flatten, concatenate, Dropout\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akash/.local/lib/python3.6/site-packages/keras/engine/saving.py:269: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "model_rnet = load_model(\"resnet50.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_rnet.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_classification = Dense(1024, activation='relu')(model_rnet.get_layer('avg_pool').output)\n",
    "model_classification = Dropout(0.5)(model_classification)\n",
    "\n",
    "model_classification = Dense(512, activation='relu')(model_classification)\n",
    "\n",
    "model_classification = Dense(36, activation='sigmoid', name='classification')(model_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'layers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-94de0d4f7668>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtop_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_classification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'layers'"
     ]
    }
   ],
   "source": [
    "top_layer = model_classification.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_regression_input = Input((224, 224, 3), name='input_regression')\n",
    "model_regression = Conv2D(16, 3)(model_regression_input)\n",
    "model_regression = MaxPool2D()(model_regression)\n",
    "\n",
    "model_regression = Conv2D(24, 5)(model_regression)\n",
    "model_regression = MaxPool2D()(model_regression)\n",
    "\n",
    "model_regression = Conv2D(32, 5)(model_regression)\n",
    "model_regression = MaxPool2D()(model_regression)\n",
    "\n",
    "model_regression = Flatten()(model_regression)\n",
    "model_regression = Dense(128)(model_regression)\n",
    "\n",
    "model_regression = concatenate([model_regression, model_classification])\n",
    "\n",
    "model_regression = Dense(1, name='regression')(model_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[model_rnet.input, model_regression_input], outputs=[model_classification, model_regression])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss={\n",
    "        'regression': 'mean_squared_error',\n",
    "        'classification': 'binary_crossentropy'\n",
    "    },\n",
    "    metrics=[\n",
    "        'accuracy'\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = ImageGenerator(image_paths_train, (image_attrs_train, image_scores_train), batch_size=128)\n",
    "test_gen = ImageGenerator(image_paths_test, (image_attrs_test, image_scores_test), batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = len(image_paths_train)\n",
    "test_len = len(image_paths_test)\n",
    "train_len, test_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit_generator(train_gen, validation_data=test_gen, epochs=200, \n",
    "                    steps_per_epoch=train_len // 128,\n",
    "                   validation_steps=10, use_multiprocessing=False,\n",
    "                   callbacks=[\n",
    "                       ReduceLROnPlateau(patience=2, verbose=1),\n",
    "                       ModelCheckpoint('chpt-4_2.hdf5', verbose=1, save_best_only=True)\n",
    "                   ])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
