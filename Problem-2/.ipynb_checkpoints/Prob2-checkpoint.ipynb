{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os.path import join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"..\\\\..\\\\Dataset-1\\\\selfie_dataset.txt\"#join(\"..\", \"..\", \"Dataset-1\", \"selfie_dataset.txt\")\n",
    "image_path = \"..\\\\..\\\\Dataset-1\\\\images\"#join(\"..\", \"..\", \"Dataset-1\", \"selfie_dataset.txt\")#join(\"..\", \"..\", \"Dataset-1\", \"images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>score</th>\n",
       "      <th>partial_faces</th>\n",
       "      <th>is_female</th>\n",
       "      <th>baby</th>\n",
       "      <th>child</th>\n",
       "      <th>teenager</th>\n",
       "      <th>youth</th>\n",
       "      <th>middle_age</th>\n",
       "      <th>senior</th>\n",
       "      <th>...</th>\n",
       "      <th>curly_hair</th>\n",
       "      <th>straight_hair</th>\n",
       "      <th>braid_hair</th>\n",
       "      <th>showing_cellphone</th>\n",
       "      <th>using_earphone</th>\n",
       "      <th>using_mirror</th>\n",
       "      <th>braces</th>\n",
       "      <th>wearing_hat</th>\n",
       "      <th>harsh_lighting</th>\n",
       "      <th>dim_lighting</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00a454da495e11e28a7322000a1fa414_6</td>\n",
       "      <td>3.901</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00cddb96ac4c11e3a30212279ba1b65f_6</td>\n",
       "      <td>4.385</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01cdd7aa1a1a11e2aaa822000a1fb0dd_6</td>\n",
       "      <td>4.243</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           image_name  score  partial_faces  is_female  baby  \\\n",
       "0  00a454da495e11e28a7322000a1fa414_6  3.901              1          1    -1   \n",
       "1  00cddb96ac4c11e3a30212279ba1b65f_6  4.385              1          1    -1   \n",
       "2  01cdd7aa1a1a11e2aaa822000a1fb0dd_6  4.243             -1          1    -1   \n",
       "\n",
       "   child  teenager  youth  middle_age  senior      ...       curly_hair  \\\n",
       "0     -1        -1      1          -1      -1      ...               -1   \n",
       "1     -1        -1     -1          -1      -1      ...               -1   \n",
       "2     -1         1     -1          -1      -1      ...               -1   \n",
       "\n",
       "   straight_hair  braid_hair  showing_cellphone  using_earphone  using_mirror  \\\n",
       "0             -1          -1                 -1              -1            -1   \n",
       "1             -1          -1                 -1              -1            -1   \n",
       "2             -1          -1                 -1              -1            -1   \n",
       "\n",
       "   braces  wearing_hat  harsh_lighting  dim_lighting  \n",
       "0      -1           -1              -1            -1  \n",
       "1      -1           -1              -1            -1  \n",
       "2      -1           -1              -1            -1  \n",
       "\n",
       "[3 rows x 38 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers = [\n",
    "    \"image_name\", \"score\", \"partial_faces\" ,\"is_female\" ,\"baby\" ,\"child\" ,\"teenager\" ,\"youth\" ,\"middle_age\" ,\"senior\" ,\"white\" ,\"black\" ,\"asian\" ,\"oval_face\" ,\"round_face\" ,\"heart_face\" ,\"smiling\" ,\"mouth_open\" ,\"frowning\" ,\"wearing_glasses\" ,\"wearing_sunglasses\" ,\"wearing_lipstick\" ,\"tongue_out\" ,\"duck_face\" ,\"black_hair\" ,\"blond_hair\" ,\"brown_hair\" ,\"red_hair\" ,\"curly_hair\" ,\"straight_hair\" ,\"braid_hair\" ,\"showing_cellphone\" ,\"using_earphone\" ,\"using_mirror\", \"braces\" ,\"wearing_hat\" ,\"harsh_lighting\", \"dim_lighting\"\n",
    "]\n",
    "df_image_details = pd.read_csv(data_path, names=headers, delimiter=\" \")\n",
    "df_image_details.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_image_details = df_image_details[df_image_details.is_female != 0]\n",
    "df_image_details.replace(to_replace=-1, value=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names = df_image_details.image_name.values.copy()\n",
    "image_scores = df_image_details[headers[1]].values.copy()\n",
    "image_attrs = df_image_details[headers[2:]].values.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = [join(image_path, iname) + '.jpg' for iname in image_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths_train, image_paths_test = image_paths[:-1000], image_paths[-1000:]\n",
    "image_attrs_train, image_attrs_test = image_attrs[:-1000], image_attrs[-1000:]\n",
    "image_scores_train, image_scores_test = image_scores[:-1000], image_scores[-1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import Sequence\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageGenerator(Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y1 = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "\n",
    "        # read your data here using the batch lists, batch_x and batch_y\n",
    "        x = [self.read_image(filename) for filename in batch_x] \n",
    "        y1 = [atrributes for atrributes in batch_y1]\n",
    "        return np.array(x), np.array(y1)\n",
    "    \n",
    "    def read_image(self, fname):\n",
    "        im = cv2.imread(fname)\n",
    "        im = cv2.resize(im, (224, 224), interpolation=cv2.INTER_CUBIC)\n",
    "        return im / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import resnet50\n",
    "from keras.layers import Dense, Conv2D, MaxPool2D, Input, Flatten, concatenate, Dropout\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"prob2.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_p = Dense(1, name='final_op')(model.get_layer('classification').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joavi\\Anaconda3\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "model2 = Model(input=[model.input[0]], output=[model_p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = ImageGenerator(image_paths_train, image_scores_train, batch_size=8)\n",
    "test_gen = ImageGenerator(image_paths_test, image_scores_test, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44227, 1000)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_len = len(image_paths_train)\n",
    "test_len = len(image_paths_test)\n",
    "train_len, test_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(optimizer=Adam(0.003), loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model2.fit_generator(train_gen, validation_data=test_gen, epochs=200, \n",
    "                    steps_per_epoch=train_len // 8,\n",
    "                   validation_steps=10, use_multiprocessing=False,\n",
    "                   callbacks=[\n",
    "                       ReduceLROnPlateau(patience=2, verbose=1),\n",
    "                       ModelCheckpoint('prob2-op.hdf5', verbose=1, save_best_only=True)\n",
    "                   ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save('prob2-op_2.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1169170_502986386472893_1145095889_a</td>\n",
       "      <td>4.009</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10175250_234210810104060_480751731_a</td>\n",
       "      <td>4.116</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10246009_608312175925674_1417074198_a</td>\n",
       "      <td>4.845</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      0      1   2   3   4   5   6   7   8   \\\n",
       "0   1169170_502986386472893_1145095889_a  4.009  -1  -1  -1  -1  -1   1  -1   \n",
       "1   10175250_234210810104060_480751731_a  4.116  -1  -1  -1  -1  -1   1  -1   \n",
       "2  10246009_608312175925674_1417074198_a  4.845  -1   1  -1  -1  -1   1  -1   \n",
       "\n",
       "   9  ...  28  29  30  31  32  33  34  35  36  37  \n",
       "0  -1 ...  -1   1  -1   1  -1   1  -1  -1  -1  -1  \n",
       "1  -1 ...  -1  -1  -1  -1  -1  -1  -1  -1   1   1  \n",
       "2  -1 ...  -1   1  -1  -1  -1   1  -1  -1  -1  -1  \n",
       "\n",
       "[3 rows x 38 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('..\\\\Test Samples.csv', header=None)\n",
    "df_test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joavi\\Anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\base.py:251: UserWarning: Trying to unpickle estimator KMeans from version 0.19.1 when using version 0.20.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "mod = pickle.load(open('km.dat', 'rb'))['clf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_columns = [\n",
    "    \"partial_faces\" ,\"is_female\" ,\"baby\" ,\"child\" ,\"teenager\" ,\"youth\" ,\"middle_age\" ,\"senior\" ,\"white\" ,\"black\" ,\"asian\" ,\"oval_face\" ,\"round_face\" ,\"heart_face\" ,\"smiling\" ,\"mouth_open\" ,\"frowning\" ,\"wearing_glasses\" ,\"wearing_sunglasses\" ,\"wearing_lipstick\" ,\"tongue_out\" ,\"duck_face\" ,\"black_hair\" ,\"blond_hair\" ,\"brown_hair\" ,\"red_hair\" ,\"curly_hair\" ,\"straight_hair\" ,\"braid_hair\" ,\"showing_cellphone\" ,\"using_earphone\" ,\"using_mirror\", \"braces\" ,\"wearing_hat\" ,\"harsh_lighting\", \"dim_lighting\"\n",
    "]\n",
    "\n",
    "di = {\n",
    "    2: 'good',\n",
    "    1: 'poor',\n",
    "    0: 'average'\n",
    "}\n",
    "\n",
    "try:\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    for i in range(len(df_test)):\n",
    "        im = df_test[0][i]\n",
    "        score_true = df_test[1][i]\n",
    "        im = cv2.imread('..\\\\Test Samples\\\\'+im+'.jpg')\n",
    "        imr = cv2.resize(im, (224, 224))\n",
    "        imr = np.expand_dims(imr, axis=0)\n",
    "        im = cv2.resize(im, (0, 0), fx=1.5, fy=1.5)\n",
    "        predictions = model2.predict(imr)\n",
    "        text = di[mod.predict(predictions)[0]] + ' ' + di[mod.predict([[score_true]])[0]]\n",
    "        \n",
    "        im = cv2.putText(im, text, (20, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0, 255), lineType=cv2.LINE_AA) \n",
    "        cv2.imshow(\"Camera\", im)\n",
    "        k = cv2.waitKey(0)\n",
    "        if k == ord('q'):\n",
    "            raise KeyboardInterrupt\n",
    "except KeyboardInterrupt:\n",
    "    cv2.destroyAllWindows()\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt = model2.get_weights()[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "s= sorted(list(zip(wt, headers[2:])))[::-1]\n",
    "\n",
    "l = [(i[0][0],i[1]) for i in s ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.4746324, 'oval_face'),\n",
       " (1.2742968, 'youth'),\n",
       " (1.0290552, 'white'),\n",
       " (0.8232053, 'wearing_hat'),\n",
       " (0.6864578, 'black_hair'),\n",
       " (0.4776673, 'is_female'),\n",
       " (0.46525386, 'brown_hair'),\n",
       " (0.45637965, 'middle_age'),\n",
       " (0.43726975, 'teenager'),\n",
       " (0.38000342, 'smiling'),\n",
       " (0.27211607, 'child'),\n",
       " (0.19715503, 'dim_lighting'),\n",
       " (0.18748626, 'using_mirror'),\n",
       " (0.17118129, 'asian'),\n",
       " (0.16632095, 'round_face'),\n",
       " (0.097300045, 'partial_faces'),\n",
       " (0.082200296, 'wearing_glasses'),\n",
       " (0.08185125, 'using_earphone'),\n",
       " (0.06477746, 'wearing_sunglasses'),\n",
       " (0.058075175, 'showing_cellphone'),\n",
       " (0.024225518, 'straight_hair'),\n",
       " (0.012132371, 'black'),\n",
       " (0.0021030207, 'wearing_lipstick'),\n",
       " (-0.0044678906, 'tongue_out'),\n",
       " (-0.007933908, 'curly_hair'),\n",
       " (-0.033131763, 'heart_face'),\n",
       " (-0.039376955, 'harsh_lighting'),\n",
       " (-0.099403135, 'duck_face'),\n",
       " (-0.1080225, 'blond_hair'),\n",
       " (-0.21266496, 'mouth_open'),\n",
       " (-0.26303416, 'frowning'),\n",
       " (-0.67733514, 'braces'),\n",
       " (-0.87619036, 'red_hair'),\n",
       " (-0.8898234, 'braid_hair'),\n",
       " (-1.3407205, 'senior'),\n",
       " (-1.3962405, 'baby')]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
